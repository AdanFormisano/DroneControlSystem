Review:
    ✔ main.cpp @done (24-08-09 18:36)
    ✔ DroneControl @done (24-08-09 18:36)
    ✔ DroneManager @done (24-08-09 18:36)
    ☐ DroneZone
    ☐ Drone
    ☐ ChargeBase
    ☐ Buffer
    ☐ Database
    ☐ TestGenerator
    ☐ Monitors

High Priority:
    ☐ Monitors:
        ✔ Each monitor thread needs to be created inside the Main function: the execution of main needs to continue (TICK doesn't get printed) @done (24-05-24 11:19)
        ✔ Translate to C++ @done (24-05-18 18:01)
        ☐ Implement CheckDroneRechargeTime:
            ✔ Implement @done (24-05-18 18:01)
            ☐ Test in C++
        ☐ Implement CheckAreaCoverage
        ☐ Implement CheckZoneVerification:
            ✔ Check if a zone-based db table is needed @done (24-08-12 17:57)
        ☐ Implement CheckTimeToReadDroneData:
            ☐ Add a new column to DB "ON_TIME" for DC's "tick_took_too_long" @Maybe
        ☐ Implement CheckDroneCharge
    ☐ Better implementation of new_drone's final_destination calculations
        (swapping drones are "jumping" to the working coords)
    ☐ System's response to scenarios:
        ✔ When a drone is removed/unavailable, DC needs to adapt to the fact that the stream doesn't have the drone's status-update @done (24-06-05 20:59)
            (Everything that uses the drone's "existence" needs to be adjusted eg. DC's and DB's usages of zone:id:drones_active/zone:id:drones_alive)
        ✔ Implement "NO_CONNECTION" version of drone's state machine: @done (24-05-22 20:10)
            ✘ Special Move() for drone w/o connection @cancelled (24-05-22 17:11)
        ✔ If a drone that is swapping loses connection DC doesn't need to send immediately a new one, it needs to wait for the outcome of the fault @done (24-06-05 20:59)
    ☐ @critical Implement if a drone ahs 0% charge it dies and needs to be deleted
    ☐ @critical Does the logdb need to have logs also for what DC does?
    ☐ SCENARIOS:
        ☐ Send new drones to the zone
    ☐ Better implementation of Drone's destruction/swap functions:
        ☐ Destruction of a drone only destroys/checks the working drone, it needs to check if because of a fault other drones need to be destroyed
    ☐ Simplify DroneZone::ManageFaults(), it's better not to have a giant single function for fault managing
    ☐ @critical Fix check value for first WORKING tick

    ✔ Check and fix drone state machine: using Redis for status reads/checks not well implemented @done (24-05-27 17:53)
    ✔ Test Generators: @done (24-05-22 20:19)
        ✔ Implement the test generator's side of all scenarios @done (24-05-22 20:19)
Normal Priority:
    ☐ Make drone_data struct model global, this makes stream data more homogeneous
    ☐ When simulation ends let the threads finish writing on DBs

Low Priority:
    ☐ Check all the debug prints, choose the one to keep and the one to put in DEBUG mode
    ☐ Check if a pool of threads is needed to read from stream in DC
    ☐ Re-evaluate the break if a tick took too long
    ☐ Implement a priority on DC for the buffer writing
    ☐ Rename CaccaPupu to decent name
    ☐ Change FAULT_SWAP status to "EMERGENCY_SWAP"
    ✔ In DroneZone create functions for swap-checking and destroying drones @done (24-05-22 17:07)

Scenarios:
    ✔ @critical Decide how the system recognizes the scenario changes: @done (24-05-15 20:19)
        - Does the drone change its status (depending on the input created by the TestGenerator) and the DC reacts to it?
        - Is the DC the one that interprets the absence of status-updates from a drone? (eg. waits ~20 ticks and then decides)

    The System first assumes that the drone has lost its connection and tries to reconnect to the drone. If the drone doesn't
    reconnect to the system, then it's assumed that the it's lost/broken and a new drone is sent.

    Everything is fine:
        Default Scenario of the simulation.

        ✔ Default state @done (24-05-15 16:32)

    Drone fault:
        This Scenario represents a _single_ drone that has stopped working and it's **not recoverable**.
        The System needs to recognize that the drone has "exploded" and send a new one.

        ✔ Chose drone to explode @done (24-05-15 16:39)
        ✔ Chosen drone has to actually stop working @done (24-05-15 19:21)
        ✔ DroneControl has to recognize that a drone has exploded: @done (24-06-05 21:02)
            ✔ An ack needs to be sent to DroneZone to start the "counting" of ticks @done (24-05-27 17:53)
            ✔ DC shares a data structure with WriteToDB thread that contains all the ticks (minibuffers) that needs to "filled" (which drones to fill for) @done (24-05-27 17:53)
            ✔ WriteToDB needs to fill the minibuffers @done (24-05-27 17:53)
            ✔ DC needs a map to have a view on which drone is not sending any status-update @done (24-05-27 17:53)
                ✔ If delta_tick > ~20 ticks then start "recover process" @done (24-05-27 17:53)
            ✔ DC needs to check when the fault management has finished (reading fault_state from redis) @done (24-06-05 21:02)
        ✔ The System need to send a new drone to the exposed zone: @done (24-06-05 21:02)
            ✔ The new drone has a special status that indicates that it's swapping a drone that has exploded @done (24-06-05 21:02)
            ✔ The new drone needs to continue from where the previous drone was @done (24-06-05 21:02)
        ✔ The logdb needs to represent what happened to the system: @done (24-05-27 17:53)
            ✔ Drone should stop sending update @done (24-05-27 17:53)
            ✔ New drone should have status "EMERGENCY_SWAP_DEAD" (or something like that) @done (24-05-27 17:53)

    Connection lost:
        This Scenario represents a _single_ drone that has lost its connection with the System but is still working.
        The System needs to first recognize the loss of connection and then try to restore the connection,
        if the attempt doesn't work consider the drone as broken.

        **Process of restoring a lost connection**
            A lost connection is indicated by a drone's status-update that takes more than a given number of
            ticks to be read by the DroneControl (eg. ~20 ticks).
                [This ability of the DC to wait for a number of ticks means that there needs to be a function
                that checks if the _last_tick_received_, for a given drone, is more the 20 ticks from the current
                tick... this can be expensive]

        **Consequences of a lost connection**
            1. In the current implementation of minibuffers, a loss of connection means that the thread writing
            the log-entries on the db will stop (a minibuffer is written to db only if all the expected entries
            are found). This means that...
                a. DC needs to manually write "empty"/special entries in every minibuffer that needs it.
                b. After the "waiting" time DC tells the thread to fill up the minibuffers that are blocked
                with "empty"/special entries.

        ✔ The system recognizes the loss of connection and tries to restore it @done (24-05-22 20:21)
        ✔ There needs to be a % for a successful reconnection @done (24-05-22 20:21)
        ✔ If failed, the System need to send a new drone to the exposed zone: @done (24-06-05 21:03)
            ✔ The new drone has a special status that indicates that it's swapping a drone that has been lost @done (24-06-05 21:03)
            ✔ The new drone needs to continue from where the previous drone was @done (24-06-05 21:03)
        ✔ The logdb needs to represent what happened to the system: @done (24-06-05 21:03)
            ✔ Drone should stop sending update @done (24-06-05 21:03)
            ✔ New drone should have status "EMERGENCY_SWAP_CONNECTION" (or something like that) @done (24-06-05 21:03)


Loop-scenarios:
    Loop-scenarios are "special" Scenarios that repeat after a given number of ticks (time) and have a probability
    of causing "damage"/changes to the environment/system.

    Dust storm:
        Every "day" at the same time (eg. 00:00) there is a chance (10%) that a drone breaks and (40%) that the storm
        causes connection loss for a number of drones. This is in fact the same scenarios the **Drone fault** and
        **Connection lost**.
        The storm has a duration, during this period the System shouldn't send a new drone because there is a chance
        that the new drone will be destroyed

        **Drone Fault**
        ☐ Chose drone that the storm destroys
        ☐ Chosen drone has to actually stop working
        ☐ DroneControl has to recognize that a drone has exploded
        ☐ The System need to send a new drone to the exposed zone:
            ☐ The new drone has a special status that indicates that it's swapping a drone that has exploded
            ☐ The new drone needs to continue from where the previous drone was
        ☐ The logdb needs to represent what happened to the system:
            - Drone should stop sending update
            - New drone should have status "EMERGENCY_SWAP_DEAD" (or something like that)

        **Connection Lost**
        ☐ The system recognizes the loss of connection and tries to restore it
        ☐ If failed, the System need to send a new drone to the exposed zone, if the storm has ended:
            ☐ The new drone has a special status that indicates that it's swapping a drone that has been lost
            ☐ The new drone needs to continue from where the previous drone was
        ☐ The logdb needs to represent what happened to the system:
            - Drone should stop sending update
            - New drone should have status "EMERGENCY_SWAP_CONNECTION" (or something like that)
